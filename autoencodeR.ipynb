{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an Autoencoder?\n",
    "\n",
    "Target audiences: Development team and Data science team.\n",
    "\n",
    "Within artificial neural networks, we can find different types of architectures.  Among theseis the autoencoder neural network.  Autoencoders are an example of self-supervised learn-ing (self-supervised learning is unsupervised learning where data provides the supervision).An autoencoder applies backpropagation.  It takes the input and maps it to a compressedrepresentation  via  an  encoder  and  then  decodes  it  back  to  an  output.   This  output  willhave the same dimensions as the input and will be a reconstruction of it.\n",
    "\n",
    "An  autoencoder  will  take  the  input,  try  to  encode  features  from  the  data,  compress  thedata  and  from  this  compressed  representation  then  give  an  output  that  will  be  a  recon-struction  of  the  original  input.   For  example,  if  you  input  the  image  of  a  person,  you  willobtain a reconstruction of that same image.\n",
    "\n",
    "In this notebook we will create a basic autoencoder using R and the MNIST dataset.\n",
    "\n",
    "To do that, we need to first import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tensorflow)\n",
    "library(keras)\n",
    "library(tidyr)\n",
    "library(ggplot2)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the data from the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist <- dataset_mnist()\n",
    "x_train <- mnist$train$x/255\n",
    "x_test <- mnist$test$x/255\n",
    "x_train <- array_reshape(x_train, c(nrow(x_train), original_dim), order = \"F\")\n",
    "x_test <- array_reshape(x_test, c(nrow(x_test), original_dim), order = \"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the following parameters. These are going to be the dimensions of the different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size       <- 100L\n",
    "original_dim     <- 784L\n",
    "latent_dim       <- 2L\n",
    "intermediate_dim <- 256L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the model:\n",
    "- x is the input layer\n",
    "- h is the hidden layer\n",
    "- z is the latent space representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- layer_input(shape = c(original_dim))\n",
    "h <- layer_dense(x, intermediate_dim, activation = \"relu\")\n",
    "z <- layer_dense(h, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder: a network that compresses the input into a latent space representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder <- keras_model(x, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder: a network that reconstructs the input from its compressed representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_h <- layer_dense(units = intermediate_dim, activation = \"relu\")\n",
    "decoder_mean <- layer_dense(units = original_dim, activation = \"sigmoid\")\n",
    "h_decoded <- decoder_h(z)\n",
    "x_decoded_mean <- decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put together the autoencoder and compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder <- keras_model(x, x_decoded_mean)\n",
    "autoencoder %>% compile(optimizer = \"rmsprop\", loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder %>% fit(\n",
    "  x_train, x_train, \n",
    "  shuffle = TRUE, \n",
    "  epochs = 50, \n",
    "  validation_data = list(x_test, x_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
